{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofjZEW19rFzK",
        "outputId": "fde12a09-ed9d-4cbc-d025-594160469886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install datasets scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "class EnhancedSBERT:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.kmeans = None\n",
        "        self.knowledge_base = []\n",
        "\n",
        "    def fine_tune(self, train_samples, epochs=1, batch_size=16):\n",
        "        \"\"\"Fine-tune the model on custom data\"\"\"\n",
        "        train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=batch_size)\n",
        "        train_loss = losses.CosineSimilarityLoss(self.model)\n",
        "\n",
        "        self.model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=epochs, warmup_steps=100)\n",
        "\n",
        "    def encode(self, sentences):\n",
        "        return self.model.encode(sentences, convert_to_numpy=True)\n",
        "\n",
        "    def find_optimal_clusters(self, embeddings, max_clusters=10):\n",
        "        \"\"\"Find optimal number of clusters using silhouette score\"\"\"\n",
        "        silhouette_scores = []\n",
        "        for n_clusters in range(2, min(max_clusters + 1, len(embeddings))):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(embeddings)\n",
        "            silhouette_scores.append(silhouette_score(embeddings, cluster_labels))\n",
        "\n",
        "        optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
        "        return optimal_clusters\n",
        "\n",
        "    def cluster_sentences(self, sentences, n_clusters=None):\n",
        "        \"\"\"Cluster sentences using K-means\"\"\"\n",
        "        embeddings = self.encode(sentences)\n",
        "        if n_clusters is None:\n",
        "            n_clusters = self.find_optimal_clusters(embeddings)\n",
        "\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "        return self.kmeans.fit_predict(embeddings)\n",
        "\n",
        "    def classify_new_sentence(self, sentence):\n",
        "        \"\"\"Classify a new sentence into existing clusters\"\"\"\n",
        "        if self.kmeans is None:\n",
        "            raise ValueError(\"Must cluster sentences before classification\")\n",
        "\n",
        "        embedding = self.encode([sentence])\n",
        "        return self.kmeans.predict(embedding)[0]\n",
        "\n",
        "    def semantic_search(self, query, corpus, top_k=5):\n",
        "        \"\"\"Perform semantic search\"\"\"\n",
        "        query_embedding = self.encode([query])\n",
        "        corpus_embeddings = self.encode(corpus)\n",
        "\n",
        "        cos_scores = np.dot(query_embedding, corpus_embeddings.T)[0]\n",
        "        top_results = np.argsort(cos_scores)[::-1][:top_k]\n",
        "\n",
        "        return [(corpus[idx], cos_scores[idx]) for idx in top_results]\n",
        "\n",
        "    def add_to_knowledge_base(self, sentences):\n",
        "        \"\"\"Add new sentences to the knowledge base\"\"\"\n",
        "        self.knowledge_base.extend(sentences)\n",
        "\n",
        "    def load_knowledge_from_file(self, file_path):\n",
        "        \"\"\"Load knowledge from a text file\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            sentences = file.readlines()\n",
        "        # Remove any leading/trailing whitespace and empty lines\n",
        "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "        self.add_to_knowledge_base(sentences)\n",
        "\n",
        "    def chatbot(self):\n",
        "        \"\"\"Interactive chatbot using semantic search\"\"\"\n",
        "        print(\"Chatbot: Hello! I'm here to help. Ask me anything or type 'quit' to exit.\")\n",
        "        while True:\n",
        "            user_input = input(\"You: \").strip()\n",
        "            if user_input.lower() == 'quit':\n",
        "                print(\"Chatbot: Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not self.knowledge_base:\n",
        "                print(\"Chatbot: I'm sorry, but my knowledge base is empty. Please add some information first.\")\n",
        "                continue\n",
        "\n",
        "            results = self.semantic_search(user_input, self.knowledge_base, top_k=2)\n",
        "\n",
        "            if results[0][1] < 0.3:  # If the best match has low similarity\n",
        "                print(\"Chatbot: I'm not sure about that. Can you please rephrase or ask something else?\")\n",
        "            else:\n",
        "                print(\"Chatbot: Based on what I know:\")\n",
        "                for sentence, score in results:\n",
        "                    print(f\"- {sentence} (Confidence: {score:.2f})\")\n",
        "if __name__ == \"__main__\":\n",
        "    enhanced_sbert = EnhancedSBERT()\n",
        "\n",
        "    # Fine-tuning example\n",
        "    train_examples = [\n",
        "        InputExample(texts=['The cat sits on the mat', 'There is a cat on the mat'], label=0.8),\n",
        "        InputExample(texts=['I love machine learning', 'I enjoy artificial intelligence'], label=0.7),\n",
        "        InputExample(texts=['Python is great for data science', 'R is also used in data analysis'], label=0.6),\n",
        "        InputExample(texts=['Neural networks are complex', 'Deep learning models can be intricate'], label=0.9),\n",
        "        InputExample(texts=['Climate change is a global issue', 'Environmental protection is crucial'], label=0.8),\n",
        "    ]\n",
        "    enhanced_sbert.fine_tune(train_examples)\n",
        "\n",
        "    # Expanded clustering example\n",
        "    sentences = [\n",
        "        \"I love machine learning and artificial intelligence\",\n",
        "        \"The cat sits on the mat while the dog sleeps\",\n",
        "        \"Python is a great programming language for data science\",\n",
        "        \"Deep learning is fascinating and has many applications\",\n",
        "        \"Dogs are loyal animals and make great pets\",\n",
        "        \"Climate change is affecting global weather patterns\",\n",
        "        \"Renewable energy sources are becoming more popular\",\n",
        "        \"The stock market fluctuates based on various factors\",\n",
        "        \"Healthy eating habits contribute to overall well-being\",\n",
        "        \"Space exploration has led to many technological advancements\",\n",
        "        \"Virtual reality is changing the gaming industry\",\n",
        "        \"Cybersecurity is crucial in the digital age\",\n",
        "        \"Artificial intelligence is used in autonomous vehicles\",\n",
        "        \"Quantum computing could revolutionize cryptography\",\n",
        "        \"Blockchain technology has applications beyond cryptocurrency\"\n",
        "    ]\n",
        "    clusters = enhanced_sbert.cluster_sentences(sentences)\n",
        "    print(\"Clusters:\", clusters)\n",
        "\n",
        "    # Print sentences grouped by cluster\n",
        "    cluster_dict = {}\n",
        "    for sentence, cluster in zip(sentences, clusters):\n",
        "        if cluster not in cluster_dict:\n",
        "            cluster_dict[cluster] = []\n",
        "        cluster_dict[cluster].append(sentence)\n",
        "\n",
        "    for cluster, sentences in cluster_dict.items():\n",
        "        print(f\"\\nCluster {cluster}:\")\n",
        "        for sentence in sentences:\n",
        "            print(f\"- {sentence}\")\n",
        "\n",
        "    # Classify new sentences\n",
        "    new_sentences = [\n",
        "        \"AI is revolutionizing industries across the globe\",\n",
        "        \"Sustainable practices are essential for environmental conservation\",\n",
        "        \"Big data analytics helps companies make informed decisions\"\n",
        "    ]\n",
        "    for sentence in new_sentences:\n",
        "        cluster = enhanced_sbert.classify_new_sentence(sentence)\n",
        "        print(f\"\\nNew sentence '{sentence}' belongs to cluster {cluster}\")\n",
        "\n",
        "    # Expanded semantic search example\n",
        "    corpus = [\n",
        "        \"Machine learning is a subset of artificial intelligence\",\n",
        "        \"Neural networks are inspired by the human brain's structure\",\n",
        "        \"Data science involves analyzing large datasets to extract insights\",\n",
        "        \"Natural language processing deals with the interaction between computers and human language\",\n",
        "        \"Computer vision enables machines to interpret and understand visual information from the world\",\n",
        "        \"Reinforcement learning is a type of machine learning where agents learn to make decisions\",\n",
        "        \"Deep learning uses multiple layers of neural networks to model complex patterns\",\n",
        "        \"Artificial intelligence aims to create systems that can perform tasks requiring human intelligence\",\n",
        "        \"Supervised learning uses labeled data to train models for prediction or classification\",\n",
        "        \"Unsupervised learning finds patterns in data without predetermined labels\",\n",
        "        \"Transfer learning applies knowledge from one task to improve performance on a related task\",\n",
        "        \"Generative AI can create new content, such as images or text, based on training data\",\n",
        "        \"Explainable AI focuses on making machine learning models more interpretable and transparent\",\n",
        "        \"Edge computing processes data near the source, reducing latency and bandwidth usage\",\n",
        "        \"Quantum machine learning combines quantum computing with machine learning algorithms\"\n",
        "    ]\n",
        "\n",
        "    queries = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"How do neural networks work?\",\n",
        "        \"What are the applications of machine learning?\",\n",
        "        \"Explain the concept of deep learning\",\n",
        "        \"What is the difference between supervised and unsupervised learning?\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        results = enhanced_sbert.semantic_search(query, corpus, top_k=3)\n",
        "        print(\"Semantic search results:\")\n",
        "        for sentence, score in results:\n",
        "            print(f\"- {sentence} (Score: {score:.4f})\")\n",
        "\n",
        "    # Load knowledge from file\n",
        "    enhanced_sbert.load_knowledge_from_file('knowledge.txt')\n",
        "\n",
        "    # Run the chatbot\n",
        "    enhanced_sbert.chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kn_rUPLas1XX",
        "outputId": "a9b3cf50-c933-4403-a708-5a8d6c91980b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters: [1 2 1 1 2 0 0 0 0 1 0 1 1 1 1]\n",
            "\n",
            "Cluster 1:\n",
            "- I love machine learning and artificial intelligence\n",
            "- Python is a great programming language for data science\n",
            "- Deep learning is fascinating and has many applications\n",
            "- Space exploration has led to many technological advancements\n",
            "- Cybersecurity is crucial in the digital age\n",
            "- Artificial intelligence is used in autonomous vehicles\n",
            "- Quantum computing could revolutionize cryptography\n",
            "- Blockchain technology has applications beyond cryptocurrency\n",
            "\n",
            "Cluster 2:\n",
            "- The cat sits on the mat while the dog sleeps\n",
            "- Dogs are loyal animals and make great pets\n",
            "\n",
            "Cluster 0:\n",
            "- Climate change is affecting global weather patterns\n",
            "- Renewable energy sources are becoming more popular\n",
            "- The stock market fluctuates based on various factors\n",
            "- Healthy eating habits contribute to overall well-being\n",
            "- Virtual reality is changing the gaming industry\n",
            "\n",
            "New sentence 'AI is revolutionizing industries across the globe' belongs to cluster 1\n",
            "\n",
            "New sentence 'Sustainable practices are essential for environmental conservation' belongs to cluster 0\n",
            "\n",
            "New sentence 'Big data analytics helps companies make informed decisions' belongs to cluster 1\n",
            "\n",
            "Query: What is artificial intelligence?\n",
            "Semantic search results:\n",
            "- Artificial intelligence aims to create systems that can perform tasks requiring human intelligence (Score: 0.7165)\n",
            "- Machine learning is a subset of artificial intelligence (Score: 0.6474)\n",
            "- Reinforcement learning is a type of machine learning where agents learn to make decisions (Score: 0.5308)\n",
            "\n",
            "Query: How do neural networks work?\n",
            "Semantic search results:\n",
            "- Neural networks are inspired by the human brain's structure (Score: 0.6427)\n",
            "- Deep learning uses multiple layers of neural networks to model complex patterns (Score: 0.5996)\n",
            "- Computer vision enables machines to interpret and understand visual information from the world (Score: 0.3881)\n",
            "\n",
            "Query: What are the applications of machine learning?\n",
            "Semantic search results:\n",
            "- Machine learning is a subset of artificial intelligence (Score: 0.6487)\n",
            "- Supervised learning uses labeled data to train models for prediction or classification (Score: 0.5396)\n",
            "- Reinforcement learning is a type of machine learning where agents learn to make decisions (Score: 0.4734)\n",
            "\n",
            "Query: Explain the concept of deep learning\n",
            "Semantic search results:\n",
            "- Deep learning uses multiple layers of neural networks to model complex patterns (Score: 0.6349)\n",
            "- Machine learning is a subset of artificial intelligence (Score: 0.4931)\n",
            "- Computer vision enables machines to interpret and understand visual information from the world (Score: 0.4369)\n",
            "\n",
            "Query: What is the difference between supervised and unsupervised learning?\n",
            "Semantic search results:\n",
            "- Unsupervised learning finds patterns in data without predetermined labels (Score: 0.5577)\n",
            "- Supervised learning uses labeled data to train models for prediction or classification (Score: 0.5478)\n",
            "- Machine learning is a subset of artificial intelligence (Score: 0.3782)\n",
            "Chatbot: Hello! I'm here to help. Ask me anything or type 'quit' to exit.\n",
            "You: what is deep learning\n",
            "Chatbot: Based on what I know:\n",
            "- Deep learning is a subset of machine learning., (Confidence: 0.75)\n",
            "- A convolutional neural network (CNN) is a class of deep neural networks used for analyzing visual imagery., (Confidence: 0.60)\n",
            "You: tell me about Data augmentation\n",
            "Chatbot: Based on what I know:\n",
            "- Data augmentation is a technique used to increase the diversity of data available for training models without collecting new data., (Confidence: 0.79)\n",
            "- An autoencoder is a type of neural network used to learn efficient codings of unlabeled data., (Confidence: 0.40)\n",
            "You: what is apple fruit\n",
            "Chatbot: I'm not sure about that. Can you please rephrase or ask something else?\n",
            "You: quit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_7MkTIy4qxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}